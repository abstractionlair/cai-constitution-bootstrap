#!/usr/bin/env python3
"""
Stage 1 DPO Training Script
Trains the base model to prefer good instruction-following responses using DPO
"""

import json
import sys
import os
import torch
from pathlib import Path
from datetime import datetime
import logging
from datasets import Dataset
from transformers import TrainingArguments
import time

# Setup paths
BASE_DIR = Path(os.getenv('CAI_BASE_DIR', '/workspace/runs/stage1_20250911_131105/code'))
ARTIFACTS_DIR = BASE_DIR / "artifacts"
CHECKPOINTS_DIR = BASE_DIR / "checkpoints"
sys.path.insert(0, str(BASE_DIR / 'scripts'))

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(ARTIFACTS_DIR / 'dpo_training.log')
    ]
)
logger = logging.getLogger(__name__)

def load_preference_pairs():
    """Load the high-quality preference pairs generated by log-probability evaluation"""
    
    pairs_file = ARTIFACTS_DIR / "logprob_preference_pairs.jsonl"
    
    with open(pairs_file) as f:
        pairs = [json.loads(line) for line in f]
    
    logger.info(f"üìù Loaded {len(pairs)} preference pairs")
    
    # Convert to DPO format
    dpo_data = []
    for pair in pairs:
        dpo_data.append({
            'prompt': pair['instruction'],
            'chosen': pair['chosen'],
            'rejected': pair['rejected'],
            'instruction_type': pair['instruction_type'],
            'confidence_margin': pair.get('confidence_margin', 0.0)
        })
    
    return dpo_data

def setup_model_and_tokenizer():
    """Setup model and tokenizer for DPO training"""
    
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from peft import LoraConfig, get_peft_model, TaskType
    from unsloth import FastLanguageModel
    
    model_name = "Qwen/Qwen2.5-32B"
    
    logger.info(f"üîß Setting up model and tokenizer: {model_name}")
    
    # Use Unsloth for efficient training
    try:
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=model_name,
            max_seq_length=2048,
            dtype=None,  # Auto-detect
            load_in_4bit=True,  # Use 4-bit for training efficiency
        )
        
        # Setup LoRA for efficient fine-tuning
        model = FastLanguageModel.get_peft_model(
            model,
            r=16,  # LoRA rank
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                          "gate_proj", "up_proj", "down_proj"],
            lora_alpha=32,
            lora_dropout=0.1,
            bias="none",
            use_gradient_checkpointing="unsloth",
            random_state=42,
            max_seq_length=2048,
        )
        
        logger.info("‚úÖ Unsloth model setup complete")
        
    except ImportError:
        logger.warning("Unsloth not available, using standard transformers")
        
        # Fallback to standard approach
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            load_in_4bit=True,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.float16
        )
        
        # Setup LoRA
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=16,
            lora_alpha=32,
            lora_dropout=0.1,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
        )
        
        model = get_peft_model(model, lora_config)
        logger.info("‚úÖ Standard transformers + LoRA setup complete")
    
    return model, tokenizer

def create_dpo_dataset(preference_pairs, tokenizer):
    """Create dataset for DPO training"""
    
    logger.info("üìä Creating DPO dataset...")
    
    # Format for DPO training
    formatted_data = []
    
    for pair in preference_pairs:
        # Create formatted prompt (instruction-style)
        formatted_prompt = f"Instruction: {pair['prompt']}\nResponse:"
        
        formatted_data.append({
            'prompt': formatted_prompt,
            'chosen': pair['chosen'],
            'rejected': pair['rejected']
        })
    
    # Create Hugging Face dataset
    dataset = Dataset.from_list(formatted_data)
    
    logger.info(f"‚úÖ Created dataset with {len(dataset)} examples")
    
    return dataset

def run_dpo_training():
    """Run DPO training"""
    
    logger.info("üöÄ Starting Stage 1 DPO training")
    
    # Load preference pairs
    preference_pairs = load_preference_pairs()
    
    # Setup model and tokenizer
    model, tokenizer = setup_model_and_tokenizer()
    
    # Create dataset
    dataset = create_dpo_dataset(preference_pairs, tokenizer)
    
    # Split dataset (80/20 train/eval)
    dataset = dataset.train_test_split(test_size=0.2, seed=42)
    train_dataset = dataset['train']
    eval_dataset = dataset['test']
    
    logger.info(f"üìä Train samples: {len(train_dataset)}, Eval samples: {len(eval_dataset)}")
    
    # Setup DPO trainer
    try:
        from trl import DPOTrainer, DPOConfig
        
        # Training arguments - Use TrainingArguments for base config
        from transformers import TrainingArguments
        
        training_args = TrainingArguments(
            output_dir=str(CHECKPOINTS_DIR / "stage1_dpo"),
            num_train_epochs=2,  # Conservative for initial training
            per_device_train_batch_size=1,  # Small batch for 32B model
            per_device_eval_batch_size=1,
            gradient_accumulation_steps=8,  # Effective batch size = 8
            learning_rate=2e-5,  # Conservative learning rate
            warmup_steps=100,
            logging_steps=10,
            eval_steps=50,
            save_steps=100,
            eval_strategy="steps",
            save_strategy="steps",
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            report_to=None,  # Disable wandb
            remove_unused_columns=False,
            dataloader_pin_memory=False,
        )
        
        # Create DPO trainer with DPO-specific parameters
        trainer = DPOTrainer(
            model=model,
            args=training_args,
            tokenizer=tokenizer,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            beta=0.1,  # DPO beta parameter
            max_length=1024,  # Max sequence length
            max_prompt_length=512,
        )
        
        logger.info("‚úÖ DPO trainer setup complete")
        
    except ImportError:
        logger.error("‚ùå TRL library not available - cannot run DPO training")
        logger.error("Please install: pip install trl")
        return None
    
    # Start training
    logger.info("üî• Starting DPO training...")
    start_time = time.time()
    
    try:
        # Train the model
        trainer.train()
        
        training_time = time.time() - start_time
        logger.info(f"‚úÖ Training completed in {training_time/60:.1f} minutes")
        
        # Save the final model
        final_model_dir = CHECKPOINTS_DIR / "stage1_dpo_final"
        trainer.save_model(str(final_model_dir))
        trainer.save_state()
        
        logger.info(f"üíæ Final model saved to {final_model_dir}")
        
        # Save training summary
        training_summary = {
            'model_name': "Qwen/Qwen2.5-32B",
            'training_method': 'DPO',
            'preference_pairs': len(preference_pairs),
            'train_samples': len(train_dataset),
            'eval_samples': len(eval_dataset),
            'epochs': 2,
            'learning_rate': 2e-5,
            'batch_size': 8,  # effective
            'training_time_minutes': training_time / 60,
            'final_model_path': str(final_model_dir),
            'timestamp': datetime.now().isoformat()
        }
        
        summary_file = ARTIFACTS_DIR / "dpo_training_summary.json"
        with open(summary_file, 'w') as f:
            json.dump(training_summary, f, indent=2)
        
        logger.info(f"üìã Training summary saved to {summary_file}")
        
        return trainer, training_summary
        
    except Exception as e:
        logger.error(f"‚ùå Training failed: {e}")
        raise

def main():
    """Main training function"""
    
    # Create checkpoints directory
    CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)
    
    try:
        trainer, summary = run_dpo_training()
        
        if trainer and summary:
            print("\n" + "="*60)
            print("üéâ STAGE 1 DPO TRAINING COMPLETE")
            print("="*60)
            print(f"Training time: {summary['training_time_minutes']:.1f} minutes")
            print(f"Preference pairs used: {summary['preference_pairs']}")
            print(f"Final model: {summary['final_model_path']}")
            print("\nFiles created:")
            print("  - Stage 1 LoRA adapter (checkpoints/stage1_dpo_final/)")
            print("  - Training logs (artifacts/dpo_training.log)")
            print("  - Training summary (artifacts/dpo_training_summary.json)")
            print("\n‚úÖ Ready for Stage 1 evaluation!")
        else:
            print("‚ùå Training failed - check logs for details")
            
    except Exception as e:
        logger.error(f"‚ùå Training script failed: {e}")
        print(f"\n‚ùå Training failed: {e}")
        raise

if __name__ == "__main__":
    main()