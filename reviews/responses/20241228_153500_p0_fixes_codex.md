# Codex Review Response: P0 Critical Fixes
Date: 2025-09-10
Request File: 20241228_153500_p0_fixes.md

## Methodology Assessment
1. CRITICAL — Cross‑run data leakage risk remains: The disjoint split/overlap assertions operate only within a single generator instance. The evaluation set is created with a fresh `Stage1DataGenerator(seed=12345)` without referencing the training pools, so instructions can overlap between training and evaluation across runs. Persist and reuse a global held‑out eval list or filter the eval set against saved train instructions.
2. CRITICAL — Template/key mismatch in instruction generation: `InstructionTemplates.GENERATION_TEMPLATES` uses `{content_type}`/`{topic}`, but `generate_generation_instructions` formats with `prompt=...`. Similarly, `RESPONSE_TEMPLATES` uses `{input}` while `generate_response_instructions` formats with `scenario=...`. This will raise `KeyError` at runtime and corrupt the dataset. Align placeholders or switch to the completion‑style templates.
3. HIGH — Evaluation prompt mismatch: In `evaluate_stage1.py`, both base and trained models are evaluated by sending the raw `instruction` string. Given a base (completion) model, this penalizes it versus a LoRA model nudged toward instruction‑following. Either (a) wrap base model inputs with the same completion‑style wrapper used in data generation, or (b) evaluate both models with completion‑style prompts and, optionally, report an instruction‑style secondary analysis.
4. MEDIUM — Few‑shot coverage for completion/critique/improvement: The few‑shot examples are minimal and mostly trivial. Add diverse, constitution‑anchored examples per instruction type (QA, completion, generation, response) and include at least one negative/near‑miss case to reduce prompt imitation artifacts.
5. MEDIUM — Constitution usage not tracked: Critiques use only the first principle. Sample principles by tag (stage/category), record `principle_id` per example, and report distribution to substantiate “principle usage” claims.
6. LOW — 8‑bit standardization is reasonable: Using 8‑bit across baseline, generation, and evaluation improves fairness and reproducibility. Add a small fp16 spot‑check (e.g., 50 items) to verify no sign flips in pass/fail outcomes.

## Scientific Validity
- ✅ Fixes address core flaws: completion‑style prompting for base model; explicit overlap assertions within pools; consistent 8‑bit precision; baseline assessment present.
- ❌ Cross‑run leakage not eliminated: Eval sets are not held‑out relative to the actual training instructions used; current assertions don’t guard across runs.
- ❌ Generation/response template bug: Placeholder mismatch can break instruction generation and bias pools.
- ❌ Evaluation fairness: Raw instruction prompts disadvantage a completion‑only base model; comparisons may overstate improvement.
- ⚠️ Constitution linkage: Critique uses only the first principle and doesn’t log principle usage; evidence that revisions are driven by the constitution is limited.
- ⚠️ Pool sizes: Reported QA 24/6, etc., are too small for robust claims; increase evaluation N per type before making strong assertions.

## Statistical Concerns
- Use paired testing on the same eval set for base vs trained:
  - Primary: McNemar’s test on binary success/failure per item; report Cohen’s h for effect size and 95% Wilson CIs for proportions.
  - Secondary: Bootstrap (10k) CIs for success rate differences and by‑type analyses; control FDR across multiple types via Benjamini–Hochberg.
- Power guidance (rough): To detect +10 pp absolute improvement (e.g., 60% → 70%) with 80% power at α=0.05 using McNemar, target ~600–800 paired items; for +15 pp, ~300–400 items. For MVP (N=100), treat results as exploratory and report CIs prominently.
- Quantization sensitivity: On a 50–100 item subset, compare 8‑bit vs fp16 outcomes; report agreement (e.g., Cohen’s κ or simple percent agreement) to show quantization doesn’t flip decisions.

## Recommendations
- Data integrity
  - Persist `data/stage1/train_instructions.jsonl` and derive an `eval_instructions.jsonl` that is explicitly filtered to be disjoint, with a saved `overlap_report.json` (0 overlaps expected). Reuse the same held‑out eval set across model comparisons.
  - Log and fix the template placeholder mismatch before further runs.
- Evaluation methodology
  - Wrap base model inputs with the same completion‑style prompt used in generation, or run a two‑protocol evaluation: completion‑style (primary) and instruction‑style (secondary) for both models, and report both.
  - Keep temperature fixed and low (e.g., 0.1) for evaluation; record all seeds and decoding params in results JSON.
- Constitution usage and evidence
  - Add principle sampling (by stage/category tag) and record `principle_id` per critique; compute and report “Principle Usage” distribution and its correlation with improvement.
  - Add ablations: (a) no‑critique, (b) shuffled/random principle text, (c) neutral principle. Show degraded outcomes relative to constitutional critiques to support causal claims.
- Metrics to track (publishable)
  - Critique Quality: % critiques with concrete, instruction‑specific issues (human spot‑check on a 50‑item stratified sample; report κ for inter‑annotator if possible).
  - Revision Success: Paired pre/post success rates with McNemar test and effect size by type.
  - Safety/Helpfulness/Truthfulness: HarmBench, MT‑Bench, TruthfulQA with bootstrap CIs; report judge agreement if using LLM judges.
  - Automation Level: Count manual interventions per 100 instructions; target <1% for “minimal human input”.
- Reproducibility
  - Save seeds, model/adapter hashes, quantization mode, and prompts used (or hashes) into `generation_summary.json` and evaluation outputs.
  - Version `constitution.yaml` and record the exact subset of principles used per example.

## Direct Answers to Scientific Questions
1. Base Model Methodology: Yes—completion‑style few‑shot prompting is appropriate for Qwen‑2.5‑32B base, but strengthen few‑shots and ensure evaluation uses a consistent prompting protocol to avoid bias.
2. Data Integrity: Current intra‑run assertions are good, but cross‑run leakage can still occur. Persist and enforce a global held‑out eval set; verify zero overlaps against the actual training instructions used.
3. Evaluation Methodology: 8‑bit standardization is fine; add a small fp16 sensitivity check and report agreement to rule out quantization artifacts.
4. Constitutional AI Validity: The fixes preserve the self‑critique → revise → DPO loop, but evidence that principles drive improvements is weak without principle logging and ablations; add both.
5. Pool Sizes Sufficiency: For robust claims, increase eval to ≥800 paired items (by type if possible). MVP N=100 is acceptable only for exploratory trends with wide CIs.

