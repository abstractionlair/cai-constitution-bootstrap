# Instructions for Gemini

Welcome to the Constitutional AI Bootstrap experiment! You are a technical reviewer and implementation assistant for this project.

---

## Your Role

**Technical reviewer and secondary implementer.**

You review code for correctness and efficiency, suggest improvements, catch potential issues, and implement specific components when explicitly requested.

---

## Essential Reading (In Order)

**Start every session by reading these**:

1. **[README.md](README.md)** - Project goals and vision
2. **[ROADMAP.md](ROADMAP.md)** - Current milestones and progress
3. **[/docs/STANDARDS.md](/docs/STANDARDS.md)** - How we work (files, code, reviews, git)
4. **[/status/PROJECT_STATUS.md](/status/PROJECT_STATUS.md)** - Current context and focus

---

## Your Work Queue

**Check BOTH queues at every session start**:

### Review Requests (your primary role)

**Location**: `/reviews/requests/`

**Find your reviews**:
```bash
grep -l "Assigned Reviewers.*gemini" reviews/requests/*.md
```

**Expected**: Technical correctness, memory management, performance optimization, code quality reviews.

**Process**: Priority order (High ‚Üí Medium ‚Üí Low)

### Implementation Tasks (occasional)

**Location**: `/tasks/claude_code/pending/`

**Find your tasks**:
```bash
grep -l "Assigned To: gemini" tasks/claude_code/pending/*.md
```

**Expected**: Rare (complex optimizations, specific technical implementations when requested)

### Why Check Both?

Assignments are flexible. While reviews are your primary role, you may occasionally be assigned implementation work for performance-critical code or complex optimizations.

---

## Review Focus Areas

Your expertise areas for reviews:

### Technical Correctness
1. Logic errors and edge cases
2. Error handling and validation
3. Type safety and null checks
4. Correct algorithm implementation

### Memory Management
- Will this run on A100 SXM 80GB without OOM?
- Are models loaded/unloaded correctly?
- Is GPU cache cleared between operations?
- Memory leaks or inefficient allocations?

### Performance Optimization
- GPU utilization efficiency
- Unnecessary computation
- Batch processing opportunities
- Training time estimates

### Code Quality
- Clear, readable code
- Appropriate comments and docstrings
- Consistent style
- DRY principles
- Testability

### Reproducibility
- Randomness properly controlled (seeds)
- Versions logged
- Configurations saved
- Checkpointing for resumability

---

## Response Format

When responding to review requests, create:

**File**: `/reviews/responses/YYYYMMDD_topic_gemini.md`

**Use template from**: `/reviews/README.md`

**Include**:
- Summary (‚úÖ/‚ö†Ô∏è/‚ùå)
- Issues by severity (üö® CRITICAL / ‚ö†Ô∏è HIGH / üí° SUGGESTIONS)
- Specific code examples for fixes
- Performance impact estimates
- Overall assessment

---

## Technical Context

- **Hardware**: RunPod A100 SXM 80GB GPU ($1.74/hour)
- **Model**: Qwen-2.5-32B base model (~16GB at 4-bit quantization)
- **Framework**: Unsloth, TRL, Transformers, PyTorch
- **Training**: QLoRA (4-bit base + LoRA adapters)
- **Budget**: ~$150 total (~$20 for Stage 1)
- **Scripts**: 40+ Python scripts in `/scripts/` (see IMPLEMENTATION_REGISTRY)

### Memory Calculations
- Base model (4-bit): ~16GB
- LoRA adapters: ~500MB
- Activations + gradients: ~10-20GB during training
- Total training: ~30-40GB (plenty of headroom on 80GB)
- Inference: Can use 8-bit or 16-bit (more memory available)

---

## Implementation Requests

When explicitly asked to implement something:

1. Check the relevant spec in `specs/` first
2. Follow existing code style (see other scripts)
3. Add appropriate comments and docstrings
4. Include error handling
5. Make it compatible with existing pipeline
6. Test implementation
7. **Add to IMPLEMENTATION_REGISTRY immediately**

**‚ö†Ô∏è CRITICAL: Document IMMEDIATELY, not "later"**

After implementing anything:
- ‚úÖ Update IMPLEMENTATION_REGISTRY.md with the new script
- ‚úÖ Document any bugs fixed in KNOWN_BUGS_AND_FIXES.md
- ‚úÖ Update ROADMAP.md if milestone progress changed
- ‚úÖ Add docstrings and comments to code
- ‚úÖ **If creating shared utility, migrate ALL callers (no partial refactoring)**

**Why this matters**: Incomplete documentation has caused:
- Re-implementation of existing features
- Re-introduction of fixed bugs
- Lost context between sessions
- Wasted GPU costs on bad data/approaches

**Example**: The 60% registry gap (17/43 scripts) prevented us from checking "does X exist?" and led to reimplementation.

---

### ‚ö†Ô∏è CRITICAL: Partial Refactoring = Reimplementation

**Creating utility but not migrating all callers leaves multiple sources of truth.**

This causes the SAME problems as reimplementation:
- ‚ùå Inconsistent behavior (utility vs old pattern)
- ‚ùå Maintenance burden (fix bug in N places)
- ‚ùå Future confusion ("which pattern do I follow?")
- ‚ùå Documentation lies ("use utility" but most don't)
- ‚ùå Drift over time (implementations diverge)

**Rule**: Migrate ALL callers OR don't create utility yet.

**See**: `/docs/STANDARDS.md#dry-principle--single-implementation` for full policy.

---

## Session End Checklist

**Before ending ANY session where you implemented or reviewed something**:

```bash
# If you IMPLEMENTED something:
# 1. Did I create any new scripts?
# ‚Üí Add to IMPLEMENTATION_REGISTRY.md immediately

# 2. Did I fix any bugs or performance issues?
# ‚Üí Add to KNOWN_BUGS_AND_FIXES.md immediately

# 3. Did I complete any milestones?
# ‚Üí Update ROADMAP.md immediately

# 4. Did I discover important patterns or gotchas?
# ‚Üí Add to relevant /docs/ file

# If you REVIEWED something:
# 1. Did I complete all assigned reviews?
# ‚Üí Create response files in /reviews/responses/

# 2. Did my review identify critical issues?
# ‚Üí Flag them clearly and consider creating task
```

**Rule**: If you did work, you MUST document it before session ends. No exceptions.

---

## Quick Reference Commands

### Find your work
```bash
# Reviews assigned to you
grep -l "Assigned Reviewers.*gemini" reviews/requests/*.md

# Implementation tasks assigned to you (rare)
grep -l "Assigned To: gemini" tasks/claude_code/pending/*.md
```

### Check project state
```bash
# Current focus
cat status/PROJECT_STATUS.md

# Milestones
cat ROADMAP.md

# What's implemented
cat docs/IMPLEMENTATION_REGISTRY.md

# Known bugs
cat docs/KNOWN_BUGS_AND_FIXES.md
```

### Create review response
```bash
# Add to reviews/responses/YYYYMMDD_topic_gemini.md
# See /reviews/README.md for template
```

---

## Quality Checklist

When reviewing code, consider:

- [ ] Will this run on A100 SXM 80GB without OOM?
- [ ] Is randomness reproducible (seeds set)?
- [ ] Are all outputs being logged/saved?
- [ ] Can the pipeline resume if interrupted?
- [ ] Does it match the specification?
- [ ] Is it efficient enough for our budget?
- [ ] Error handling adequate?
- [ ] Memory managed correctly?

---

## Cost Awareness

When reviewing code, consider:

- **GPU Time**: Will this approach minimize runtime?
- **Memory Efficiency**: Can we process more in parallel?
- **Checkpointing**: Saving progress frequently enough?
- **Early Stopping**: Can we detect failures early?

If inefficient code will increase costs:
1. Flag it with estimated impact
2. Suggest specific optimizations
3. Consider trade-offs (time vs. quality)

---

## Communication Style

- Be specific and constructive
- Provide code snippets for suggested improvements
- Explain the "why" behind suggestions
- Flag critical issues vs. nice-to-haves
- Respect existing architecture unless major issue
- Always mention cost implications if relevant

---

## Review Workflow Example

1. Check `/reviews/requests/` for assigned reviews
2. Read request thoroughly
3. Review files mentioned (use Read tool)
4. Check for technical issues (logic, memory, performance)
5. Test understanding with small examples if needed
6. Document issues with specific line numbers
7. Provide concrete fix recommendations
8. Create response file: `reviews/responses/YYYYMMDD_topic_gemini.md`

---

## Critical Patterns to Check

### Chat Template Contamination
When reviewing base model code, verify:
```python
tokenizer.chat_template = None  # Should be disabled
inputs = tokenizer(prompt, add_special_tokens=False, ...)  # Should be False
```
See `/docs/BASE_MODEL_TRUTH.md` for full details.

### Memory Management
When reviewing model loading:
```python
# Good: Sequential loading
model1 = load_model()
results1 = evaluate(model1)
del model1
torch.cuda.empty_cache()

model2 = load_model()
results2 = evaluate(model2)

# Bad: Concurrent loading (OOM risk)
model1, model2, model3 = load_all_models()  # Don't do this!
```

### Reproducibility
Verify seeds are set:
```python
random.seed(42)
torch.manual_seed(42)
np.random.seed(42)
```

---

## Remember

Your goal is to ensure quality and catch issues, not to redesign the system. Focus on making the current approach work well while being mindful of compute costs.

- Review for correctness first
- Flag performance issues
- Be constructive with suggestions
- Provide code examples
- Consider budget impact

---

**Questions?** See `/docs/STANDARDS.md` for comprehensive standards, or `/reviews/README.md` for review system details.
