# Data Generation Architecture: Stage 1 SFT Training Data

## Overview
This document explains our SFT (Supervised Fine-Tuning) data generation approach for Stage 1, which uses sophisticated completion-style prompting with the Qwen-2.5-32B base model.

## Critical Finding
GPT-5's analysis of our session logs was **correct**. We did implement a sophisticated few-shot completion-style approach, but we have a critical bug where we don't use it consistently.

## Two-Pronged Implementation

### 1. The Sophisticated Approach (What We Designed)

**Location**: `/scripts/utils/data_formatter.py` - `CompletionStylePrompts` class

**Method**: `create_response_generation_prompt(instruction: str) -> str`

This creates few-shot completion prompts like:
```
Here are examples of prompts and their completions:

The answer to "What is 2+2?" is:
4

Complete this sentence: The sun rises in the
east

Here is a fact about dogs:
Dogs are loyal and friendly companions to humans.

Write a short sentence about winter:
Snow blankets the quiet landscape in pristine white.

Explain what photosynthesis is:
Photosynthesis is the process by which plants convert sunlight into energy.

What is gravity?
```

**Key Features**:
- Uses 3-4 randomly selected examples for diversity
- Natural completion patterns (no "Instruction:" scaffolding)
- Teaches the base model through pattern matching
- Examples cover different instruction types (math, completion, facts, creative, explanatory)

### 2. Where We Use It Correctly

**Location**: `/scripts/stage1_generate.py` - Line 196

```python
# Use completion-style prompting for base model  
formatted_prompt = CompletionStylePrompts.create_response_generation_prompt(instruction)
```

This script properly uses our sophisticated few-shot approach.

### 3. The Bug (Where We Don't Use It)

**Location**: `/scripts/generate_stage1_sft_data.py`

**Line 268**: ‚úÖ **CORRECT** - Uses completion-style prompt for model generation:
```python
completion_prompt = self.create_completion_prompt(instruction, inst_type)
```

**Line 402**: ‚ùå **BUG** - Stores wrong prompt format in dataset:
```python
'prompt': f"Instruction: {inst_data['instruction']}\nResponse:",
```

**Result**: 
- Model generation uses completion-style prompts like `"Q: What is gravity? A:"`
- But dataset stores instruction-style prompts like `"Instruction: What is gravity?\nResponse:"`

## What We Actually Generated

The 200 examples in `sft_training_data_20250913_005116.jsonl` were generated with:

**‚úÖ GOOD**:
- Chat template disabled: `tokenizer.chat_template = None`  
- Special tokens prevented: `add_special_tokens=False`
- Base model responses (no instruction-following contamination)

**‚ùå INCONSISTENT**:
- Used completion-style prompts for generation: `"Q: What is gravity? A:"`
- But stored instruction-style prompts in dataset: `"Instruction: What is gravity?\nResponse:"`

## Why It Still Worked

Even with instruction-style prompts, the generated responses are high-quality because:
1. **Chat template properly disabled** - No instruction template applied by tokenizer
2. **No special token contamination** - Pure base model behavior  
3. **Qwen-2.5-32B inherent capabilities** - Strong base model with instruction-following abilities

## The Fix Needed

Change line 402 in `/scripts/generate_stage1_sft_data.py` from:
```python
'prompt': f"Instruction: {inst_data['instruction']}\nResponse:",
```

To:
```python  
'prompt': self.create_completion_prompt(instruction, inst_type),
```

This would store the actual completion-style prompts that were used for generation.

## Key Scripts Reference

| Script | Purpose | Uses CompletionStylePrompts? | Status |
|--------|---------|----------------------------|---------|
| `utils/data_formatter.py` | Contains CompletionStylePrompts class | N/A - Defines it | ‚úÖ Complete |
| `stage1_generate.py` | Response generation | ‚úÖ Yes (line 196) | ‚úÖ Correct |
| `generate_stage1_sft_data.py` | SFT dataset creation | ‚ùå Partial (bug line 402) | üêõ Has bug |

## Chat Template Contamination Fix (COMPLETED)

We successfully fixed the critical chat template contamination issue:

**Location**: `/scripts/generate_stage1_sft_data.py` - Line 130
```python
# Disable chat template to ensure pure base model behavior
self.tokenizer.chat_template = None
```

**Location**: Line 273  
```python
inputs = self.tokenizer(
    completion_prompt,
    add_special_tokens=False,  # Prevent template injection
    return_tensors="pt",
    max_length=512,
)
```

This ensures the base model receives raw text without any instruction-following scaffolding.

## Summary

1. **We DID implement sophisticated few-shot completion-style prompting** - GPT's analysis was correct
2. **Our chat template fix works perfectly** - Base model behavior is clean
3. **We have one critical bug** - Line 402 stores wrong prompt format in dataset
4. **The generated data quality is good** - Even with the bug, because base model handling is correct